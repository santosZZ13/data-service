Sequence diagram for fetching sofa matches by date with caching mechanism (with analytics histories matches).

1. Fetch matches for a specific date from Sofa API or cache (2 calls max).
2. Caching mechanism:
    * If cache hit, return cached data.
    * If cache miss, query Sofa API, store in cache, and return data.
3. Extract unique match IDs from fetched data.
    * Steps:
        1. Not fetched history matches immediately.
        2. Checking DB/Cache for existing history matches using team id in unique match IDs.
        3. For cache misses (outdated/missing):
            - return partial data: Matches list with analytics placeholders (null/processing)
            - At the same time, send job to Kafka for processing missing history matches for teamIds missing in cache/DB.

4. Background job processing:
    * Consume Kafka jobs for missing history matches.
    * Fetch history matches from Sofa API.
    * Store fetched history matches in DB/Cache.
    * Update analytics placeholders in matches list.

    1. Producer (From API):
        - Identify missing history matches.
        - Send message to Kafka Topic (for example: "team-histories-fetch") with teamIds.
        - Message Payload Example:
          {
            "teamIds": [123, 456, 789], // List of team IDs needing history matches
            "date": "2024-01-01" // Date for which matches were requested
            requestId: "uuid-v4" // UUID for tracing
          }
    2. Consumer (Background Worker):
        - Listen to "team-histories-fetch" Kafka Topic.
        - Fetch history matches from Sofa API for each teamId.
        - Store fetched history matches in DB/Cache. (team_histories collection/table)
        - Update analytics placeholders in matches list.
        - ** (optional) Send notification via WebSocket/Push Notification if needed. (for example: /api/sofa/analytics-updated?requestId=uuid-v4 each 5-10 seconds to check status)
    3. Polling/Update to Client (optional):
        - Client can poll /api/sofa/analytics-updated?requestId=uuid-v4 to check if analytics data is ready.
        - Once ready, client can fetch updated matches with analytics data.
        - Or use WebSocket/Push Notification to notify client when analytics data is ready.

5. Handle Update & Edge Cases
    1. Outdate data: add field "lastUpdated" timestamp in cache/DB to track data freshness. >1 day consider outdated. (re-queue to refresh)
    2. Error Handling: if fetch fail(rate limit, network), retry in consumer (use Kafka retry topic or Spring Retry)
    3. Pre-warming Cache: periodically pre-fetch popular team histories to reduce latency.
    4. Scale: Consumer can run multiple instances to handle high volume of requests.
Questions
1. How to check cache hit or miss?



@startuml
actor User
participant "Client" as FE
participant "DataService" as API
participant "Redis" as Cache
participant "MongoDB" as DB
participant "Sofa API" as Sofa
participant "Kafka" as Kafka
participant "Consumer" as Worker

User -> FE: GET /api/sofa/matches?date=2025-11-09
FE -> API: GET /matches?date=...
API -> Cache: GET matches:date:2025-11-09
alt Cache Miss
    Cache --> API: null
    API -> Sofa: GET scheduled-events + inverse
    Sofa --> API: 6000 matches
    API -> Cache: SET matches:date:2025-11-09
    API -> DB: saveToDB()
end

API -> API: extract teamIds â†’ 1000 IDs
loop for each teamId
    API -> Cache: GET team:history:123
    alt Cache Miss
        Cache --> API: null
        API -> DB: findByTeamId(123)
        alt DB Miss or Outdated
            DB --> API: null or old
            API -> API: add to missingTeamIds
        else DB Hit
            DB --> API: history
            API -> Cache: SET team:history:123
        end
    end
end

alt missingTeamIds not empty
    API -> Kafka: SEND team-histories-fetch {requestId, date, teamIds}
    API --> FE: 200 {status: "processing", requestId, matches[], analysis: null}
else
    API -> API: analyze all matches
    API --> FE: 200 {status: "completed", matches + analysis}
end

Kafka -> Worker: consume job
Worker -> Sofa: getHistoriesByTeamIds()
Sofa --> Worker: histories
Worker -> Worker: analyze
Worker -> DB: save team_histories
Worker -> Cache: SET team:history:*

note right: Client poll /analysis/status?requestId=... every 5s

@enduml